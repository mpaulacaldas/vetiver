---
title: "Model Card: Chicago inspections with rstats"
date: '`r Sys.Date()`'
output: 
  html_document
params:
    board: !r pins::board_folder("pin-warehouse", versioned = TRUE)
    name: chicago-inspections-rstats
    version: NULL
    author: "Jane Doe"
    email: "jane@doe.com"
---

```{r setup, include=FALSE}
library(tidyverse)
library(vetiver)
library(pins)
library(yardstick)
knitr::opts_chunk$set(echo = FALSE, error = TRUE)
v <- vetiver_pin_read(params$board, params$name, version = params$version)
v_meta <- pin_meta(params$board, params$name)
theme_set(theme_light())
```

A [model card](https://doi.org/10.1145/3287560.3287596) provides brief, transparent, responsible reporting for a trained machine learning model.

## Model details

- Developed by `r params$author`
- `r cli::pluralize("{v$description} using {ncol(v$prototype)} feature{?s}")`
- More details about how model was developed and what it is predicting
- More details on feature engineering and/or data preprocessing for model
- Version `r v$metadata$version` of this model was published at `r v_meta$created`
- Citation and/or license details for the model
- If you have questions about this model, please contact `r params$email`

## Intended use

- The primary intended uses of this model are ...
- The primary intended users of this model are ...
- Some use cases are out of scope for this model, such as ...

## Important aspects/factors

- Aspects or factors (demographic, environmental, technical) that are relevant to the context of this model are ...
- In evaluating this model, we examined aspects such as ...

## Metrics

- The metrics used to evaluate this model are ...
- These metrics are computed via ...
- We chose these metrics because ...

## Training data & evaluation data

- The training dataset for this model was ...
- The training dataset for this model has the "prototype" or signature:

```{r}
glimpse(v$prototype)
```

- The evaluation dataset used in this model card is ...
- We chose this evaluation data because ...

```{r}
## EVALUATION DATA:
library(arrow)
library(rsample)

set.seed(123)
inspections <- read_parquet(here::here("data", "inspections.parquet"))
inspect_split <- initial_split(inspections, prop = 0.8)
inspect_test <- testing(inspect_split)

## consider using a package like skimr or DataExplorer for automated 
## presentation of evaluation data characteristics
```


## Quantitative analyses {.tabset}

```{r}
## compute predictions for your evaluation data
## load packages needed for prediction:
library(parsnip)
library(workflows)

preds <- augment(v, inspect_test)
```


### Overall model performance

```{r}
preds %>%
  metrics(results, .pred_class)
```

### Disaggregated model performance

```{r}
preds %>%
  group_by(facility_type) %>%
  metrics(results, .pred_class)
```

### Visualize model performance

```{r, fig.height=3}
preds %>%
  count(facility_type, results, .pred_class) |> 
  ggplot(aes(results, .pred_class, color = facility_type, size = n)) +
  geom_point() +
  guides(color = "none") +
  facet_wrap(vars(facility_type))
```

### Make a custom plot

```{r}
preds %>%
  count(facility_type, results, .pred_class) |> 
  ggplot(aes(results, n, fill = .pred_class)) +
  geom_col() +
  facet_wrap(vars(facility_type))
```


## Ethical considerations

- We considered ...

## Caveats & recommendations

- This model does ...
- This model does not ...
- We recommend ...



